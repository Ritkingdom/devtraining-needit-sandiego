{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ//QeC6UtBkmGr3w7Vvwc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ritkingdom/devtraining-needit-sandiego/blob/main/Untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4XMW3Y5B-69a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dfb47b3-a559-43b0-ee57-864961414fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           candidateAId                          candidateBId  \\\n",
            "0  8ab47434-09a9-44e6-8c77-f9fd20c57765  d7cbd002-5423-4dae-82d9-3a629ec361bb   \n",
            "1  53c11bf9-3ec7-4909-a9d1-487692e72415  e957aff1-583b-11ef-8a84-4201ac164110   \n",
            "2  4617b14d-ca26-11ee-a4ba-42010a400021  a2d2933e-c5bb-11ee-a4ba-42010a400021   \n",
            "3  c227ffa7-c459-11ee-a4ba-42010a400021  e0abf437-c7b8-11ee-a4ba-42010a400021   \n",
            "4  fd4e9be6-c4f2-11ee-a4ba-42010a400021  264bd6d6-cca8-11ee-a4ba-42010a400021   \n",
            "\n",
            "                               winnerId  \\\n",
            "0  8ab47434-09a9-44e6-8c77-f9fd20c57765   \n",
            "1  e957aff1-583b-11ef-8a84-4201ac164110   \n",
            "2  4617b14d-ca26-11ee-a4ba-42010a400021   \n",
            "3  c227ffa7-c459-11ee-a4ba-42010a400021   \n",
            "4  fd4e9be6-c4f2-11ee-a4ba-42010a400021   \n",
            "\n",
            "                                candidateATranscript  \\\n",
            "0  {'pairs': [['Interviewer: Hello and welcome to...   \n",
            "1  {'pairs': [['Interviewer: Hello and welcome to...   \n",
            "2  {'pairs': [['Interviewer: Hello and welcome to...   \n",
            "3  {'pairs': [['Interviewer: Hello and welcome to...   \n",
            "4  {'pairs': [['Interviewer: Hello and welcome to...   \n",
            "\n",
            "                                candidateBTranscript  \\\n",
            "0  {'pairs': [['Interviewer: Hello and welcome to...   \n",
            "1  {'pairs': [['Interviewer: Hello! This is a sho...   \n",
            "2  {'pairs': [['Interviewer: Hello and welcome to...   \n",
            "3  {'pairs': [['Interviewer: Hello and welcome to...   \n",
            "4  {'pairs': [['Interviewer: Hello and welcome to...   \n",
            "\n",
            "                                    candidateAResume  \\\n",
            "0  {\"data\": {\"awards\": [], \"certifications\": [], ...   \n",
            "1  {\"data\": {\"awards\": [], \"certifications\": [], ...   \n",
            "2  {\"data\": {\"awards\": [\"1st (Winner) AIR 8\", \"Am...   \n",
            "3  {\"data\": {\"awards\": [], \"certifications\": [\"Ma...   \n",
            "4  {\"data\": {\"awards\": [], \"certifications\": [], ...   \n",
            "\n",
            "                                    candidateBResume  \\\n",
            "0  {\"data\": {\"awards\": [], \"certifications\": [], ...   \n",
            "1  {\"data\": {\"awards\": [], \"certifications\": [], ...   \n",
            "2  {\"data\": {\"awards\": [], \"certifications\": [\"In...   \n",
            "3  {\"data\": {\"awards\": [\"Data Science Certificate...   \n",
            "4  {\"data\": {\"awards\": [], \"certifications\": [\"AW...   \n",
            "\n",
            "                                                role  \n",
            "0                                     communications  \n",
            "1                                         ops-or-gtm  \n",
            "2                          has-scraping-experience-a  \n",
            "3                                     ml-engineer-v3  \n",
            "4  full-stack-engineer-with-experience-in-next-an...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/sample_data/train_dataset.csv')\n",
        "\n",
        "# Check the first few rows of the data\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training and testing data from CSV files\n",
        "train_data = pd.read_csv('/content/sample_data/train_dataset.csv')\n",
        "test_data = pd.read_csv('/content/sample_data/test_dataset.csv')\n",
        "\n",
        "# Define text cleaning function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = text.split()  # Split into words\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatize and remove stop words\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply cleaning to the transcripts and resume data in training set\n",
        "train_data['clean_candidateA_transcript'] = train_data['candidateATranscript'].apply(clean_text)\n",
        "train_data['clean_candidateB_transcript'] = train_data['candidateBTranscript'].apply(clean_text)\n",
        "train_data['clean_candidateA_resume'] = train_data['candidateAResume'].apply(clean_text)\n",
        "train_data['clean_candidateB_resume'] = train_data['candidateBResume'].apply(clean_text)\n",
        "\n",
        "# Combine transcript and resume data for each candidate in training set\n",
        "train_data['candidateA_combined'] = train_data['clean_candidateA_transcript'] + ' ' + train_data['clean_candidateA_resume']\n",
        "train_data['candidateB_combined'] = train_data['clean_candidateB_transcript'] + ' ' + train_data['clean_candidateB_resume']\n",
        "\n",
        "# Apply cleaning to the transcripts and resume data in testing set\n",
        "test_data['clean_candidateA_transcript'] = test_data['candidateATranscript'].apply(clean_text)\n",
        "test_data['clean_candidateB_transcript'] = test_data['candidateBTranscript'].apply(clean_text)\n",
        "test_data['clean_candidateA_resume'] = test_data['candidateAResume'].apply(clean_text)\n",
        "test_data['clean_candidateB_resume'] = test_data['candidateBResume'].apply(clean_text)\n",
        "\n",
        "# Combine transcript and resume data for each candidate in testing set\n",
        "test_data['candidateA_combined'] = test_data['clean_candidateA_transcript'] + ' ' + test_data['clean_candidateA_resume']\n",
        "test_data['candidateB_combined'] = test_data['clean_candidateB_transcript'] + ' ' + test_data['clean_candidateB_resume']\n"
      ],
      "metadata": {
        "id": "MOPZXb5cVMXM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# Fit TF-IDF on the combined candidate data from the training set\n",
        "all_combined_text_train = pd.concat([train_data['candidateA_combined'], train_data['candidateB_combined']])\n",
        "tfidf.fit(all_combined_text_train)\n",
        "\n",
        "# Transform candidate data using the fitted TF-IDF\n",
        "candidateA_features_train = tfidf.transform(train_data['candidateA_combined'])\n",
        "candidateB_features_train = tfidf.transform(train_data['candidateB_combined'])\n",
        "candidateA_features_test = tfidf.transform(test_data['candidateA_combined'])\n",
        "candidateB_features_test = tfidf.transform(test_data['candidateB_combined'])\n",
        "\n",
        "# Calculate the difference between candidate A and candidate B features for training and testing\n",
        "feature_difference_train = candidateA_features_train - candidateB_features_train\n",
        "feature_difference_test = candidateA_features_test - candidateB_features_test\n"
      ],
      "metadata": {
        "id": "jPkvppy-XPAC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target variable for training (1 if candidate A is preferred, 0 otherwise)\n",
        "y_train = (train_data['winnerId'] == train_data['candidateAId']).astype(int)\n",
        "\n",
        "# Define the target variable for testing (1 if candidate A is preferred, 0 otherwise)\n",
        "y_test = (test_data['winnerId'] == test_data['candidateAId']).astype(int)\n",
        "\n",
        "# Initialize and train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(feature_difference_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(feature_difference_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iguID_7mXWck",
        "outputId": "91eb43e3-c32d-41b9-82e8-2760c6b6f701"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.22      0.34        68\n",
            "           1       0.63      0.96      0.76        96\n",
            "\n",
            "    accuracy                           0.65       164\n",
            "   macro avg       0.71      0.59      0.55       164\n",
            "weighted avg       0.70      0.65      0.59       164\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for imbalance in the roles\n",
        "role_counts = data['role'].value_counts()\n",
        "print(role_counts)\n",
        "\n",
        "# Check if there's a significant imbalance in transcript lengths\n",
        "data['candidateA_transcript_length'] = data['candidateATranscript'].apply(len)\n",
        "data['candidateB_transcript_length'] = data['candidateBTranscript'].apply(len)\n",
        "\n",
        "transcript_length_diff = abs(data['candidateA_transcript_length'] - data['candidateB_transcript_length'])\n",
        "print(transcript_length_diff.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgeg2wn8XbMH",
        "outputId": "8a8bfa0a-f275-4aa6-f4a7-a2b765b126dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "role\n",
            "communications                                                  3\n",
            "ops-or-gtm                                                      3\n",
            "headhunter-or-recruiter                                         2\n",
            "writer                                                          2\n",
            "technical-project-manager-v3                                    2\n",
            "has-scraping-experience-a                                       1\n",
            "ml-engineer-v3                                                  1\n",
            "full-stack-engineer-with-experience-in-next-and-typescript-a    1\n",
            "backend-engineer-who-s-worked-at-a-startup-v3                   1\n",
            "leetcode-expert-b                                               1\n",
            "leetcode-expert-a                                               1\n",
            "financial-advisor                                               1\n",
            "marketer                                                        1\n",
            "Name: count, dtype: int64\n",
            "count      20.000000\n",
            "mean     2672.200000\n",
            "std      1922.317286\n",
            "min       156.000000\n",
            "25%       689.000000\n",
            "50%      2834.000000\n",
            "75%      4180.000000\n",
            "max      7339.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Using role-specific weights for balanced learning\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# Retrain model with class weights\n",
        "model_weighted = LogisticRegression(class_weight=class_weight_dict)\n",
        "model_weighted.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the weighted model\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "print(f\"Weighted Model Accuracy: {accuracy_weighted:.2f}\")\n",
        "print(classification_report(y_test, y_pred_weighted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shOy0gGxXg3g",
        "outputId": "c82b0e5f-422e-46d7-eb8d-d8e58a441fab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted Model Accuracy: 0.75\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.50      0.67         2\n",
            "           1       0.67      1.00      0.80         2\n",
            "\n",
            "    accuracy                           0.75         4\n",
            "   macro avg       0.83      0.75      0.73         4\n",
            "weighted avg       0.83      0.75      0.73         4\n",
            "\n"
          ]
        }
      ]
    }
  ]
}